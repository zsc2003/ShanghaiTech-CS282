\textcolor{blue}{Problem 1}
For logistic regression, show that

\begin{equation}
    \nabla E_{\text {in }}(\mathbf{w})  =-\frac{1}{N} \sum_{n=1}^{N} \frac{y_{n} \mathbf{x}_{n}}{1+e^{y_{n} \mathbf{w}^{\mathrm{T}} \mathbf{x}_{n}}}  
 =\frac{1}{N} \sum_{n=1}^{N}-y_{n} \mathbf{x}_{n} \theta\left(-y_{n} \mathbf{w}^{\mathrm{T}} \mathbf{x}_{n}\right)
\end{equation}
Argue that a `misclassified' example contributes more to the gradient than a correctly classified one.
\textcolor{blue}{Solution}















\newpage