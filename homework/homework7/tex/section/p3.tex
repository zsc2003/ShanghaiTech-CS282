\textcolor{blue}{Problem 3}
\par \textbf{Definition 1 (leave-one-out cross-validation)} \textit{Select each training example in turn as the single example to be held-out, train the classifier on the basis of all the remaining training examples, test the resulting classifier on the held-out example, and count the errors.}
\par Let the superscript -i denote the parameters we would obtain by finding the SVM classifier $f$ without the $i$th training example. Define the \textit{leave-one-out CV error} as
\begin{equation}
    \frac{1}{n}\sum_{i=1}^n\mathcal{L}(y_i,f(\mathbf{x}_i;\mathbf{w}^{-i},b^{-i})),
\end{equation}
where $\mathcal{L}$ is the zero-one loss. Prove that
\begin{equation}
    \text{leave-one-out CV error}\leq \frac{\text{number of support vectors}}{\text{n}}
\end{equation} (20pt) \\
\textcolor{blue}{Solution}

















\newpage